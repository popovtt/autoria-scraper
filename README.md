# AutoRia Scraper

An asynchronous web scraper for collecting car listings from **auto.ria.com** with automatic storage in PostgreSQL, database migrations via Alembic, and containerized execution using Docker.

---

## ğŸš€ Features

* Asynchronous scraping of listing pages
* Anti-blocking protection (random delays + retries)
* Duplicate prevention (URL uniqueness)
* Bulk inserts into PostgreSQL
* Structured logging
* Automatic database migrations
* Dockerized environment
* Scheduler support for timed execution

---

## ğŸ§± Tech Stack

* Python 3.13+
* aiohttp
* BeautifulSoup4
* SQLAlchemy (Async ORM)
* PostgreSQL
* Alembic
* Docker + Docker Compose

---

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ migrations/
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

---

## âš™ï¸ Environment Variables

Create a `.env` file in the project root:

```
POSTGRESQL_DSN=postgresql+asyncpg://postgres:postgres@db:5432/autoria
START_URL=https://auto.ria.com/uk/car/used/
SCRAPE_TIME=12:00
```

---

## ğŸ³ Run with Docker

### First run

```
docker compose up --build
```

### Run in background

```
docker compose up -d --build
```

---

## ğŸ—„ Database Migrations

Migrations are executed automatically through the **migrations service**.

Manual run:

```
docker compose run --rm migrations
```

Check migration status:

```
docker exec -it autoria_db psql -U postgres -d autoria
SELECT * FROM alembic_version;
```

---

## ğŸ“Š Check Database Records

```
docker exec -it autoria_db psql -U postgres -d autoria
SELECT COUNT(*) FROM car;
```

---

## ğŸ§ª Logs

```
docker compose logs -f app
docker compose logs -f migrations
docker compose logs -f db
```

---

## ğŸ›‘ Stop Services

```
docker compose down
```

Reset database completely:

```
docker compose down -v
```

---

## âš ï¸ Technical Notes

* AutoRia is a SPA website â†’ many fields are stored inside JSON state, not static HTML
* Scraper uses randomized delays and retry logic to avoid blocking
* Table must contain a UNIQUE constraint on `url`
* Docker environment uses internal hostname `db` instead of `localhost`

---

## ğŸ“ˆ Container Startup Flow

```
Postgres
   â†“
Migrations
   â†“
Application
```

The app will not start until migrations finish successfully.

---

## ğŸ§  Performance Optimizations

The scraper implements:

* concurrency limiting via semaphore
* retry strategy with backoff
* randomized request delay
* async HTTP requests
* async database driver
* bulk insert operations

---

## ğŸ§¾ Data Model

| Field          | Type     |
| -------------- | -------- |
| url            | string   |
| title          | string   |
| price_usd      | integer  |
| odometer       | integer  |
| username       | string   |
| phone_number   | string   |
| image_url      | string   |
| images_count   | integer  |
| car_number     | string   |
| car_vin        | string   |
| datetime_found | datetime |

---

## ğŸ— Architecture Overview

```
Scraper â†’ Parser â†’ Validation â†’ Database â†’ Scheduler
```

**Flow explanation**

1. Scraper loads listing page
2. Extracts car URLs
3. Fetches each car page asynchronously
4. Parses data
5. Filters duplicates
6. Inserts batch into DB

---

## ğŸ”’ Anti-Blocking Strategy

The scraper reduces detection risk using:

* randomized delays
* realistic headers
* retry with exponential backoff
* limited concurrency
* human-like request pattern

---

## ğŸ§ª Development Mode (without Docker)

```
pip install -r requirements.txt
alembic upgrade head
python -m src.main
```

---

## ğŸ§© Troubleshooting

### Database connection fails

Check `.env` DSN and container logs.

### Tables missing

Run:

```
docker compose run --rm migrations
```

### Port already in use

Remove port mapping from Postgres or change host port.

---

## ğŸ‘¨â€ğŸ’» Author

Roman Popov
